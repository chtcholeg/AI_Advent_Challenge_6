# Описание AI моделей

Документ содержит описание всех моделей, используемых в приложении.

## GigaChat (Sberbank)

### GigaChat (Базовая)
- **ID**: `GigaChat`
- **Дата выхода**: Апрель 2023 (закрытое тестирование), публичный релиз позже в 2023
- **Разработчик**: Sberbank (Россия)
- **Описание**: Мультимодальная нейросеть, российская альтернатива ChatGPT. Генерирует текст, отвечает на вопросы, создаёт изображения по описанию, пишет код.
- **Задачи**: Общение на русском языке, генерация текста, ответы на вопросы, написание кода, создание изображений.
- **Особенности**: Оптимизирована для русского языка, соответствует российским правовым требованиям.

### GigaChat Pro
- **ID**: `GigaChat-Pro`
- **Дата выхода**: 2024 (GigaChat 2.0 - Март 2025)
- **Разработчик**: Sberbank (Россия)
- **Описание**: Улучшенная версия GigaChat с качеством предыдущей версии MAX. Баланс между производительностью и ресурсоёмкостью.
- **Задачи**: Задачи, требующие креативности и точности, сложные диалоги, генерация контента.
- **Особенности**: Менее требовательна к ресурсам при сохранении высокого качества.

### GigaChat Max
- **ID**: `GigaChat-Max`
- **Дата выхода**: 2024 (GigaChat 2.0 - Март 2025)
- **Разработчик**: Sberbank (Россия)
- **Описание**: Самая мощная модель линейки GigaChat. Первое место в бенчмарке MERA для русского языка.
- **Задачи**: Обработка больших текстов (до ~200 страниц), программирование, анализ данных, фактологические вопросы на русском.
- **Особенности**: Превосходит DeepSeek-V3, Qwen2.5, GPT-4o и LLaMA 70B по качеству ответов на русском языке.

---

## Hugging Face Models (выбранные для приложения)

Для приложения выбраны 5 моделей из разных временных периодов для демонстрации эволюции открытых LLM (от сентября 2023 до апреля 2025).

### Mistral 7B Instruct v0.1 (САМАЯ СТАРАЯ)
- **ID**: `mistralai/Mistral-7B-Instruct-v0.1`
- **Дата выхода**: 27 сентября 2023
- **Разработчик**: Mistral AI (Франция)
- **Параметры**: 7 миллиардов
- **Описание**: Первая открытая модель от Mistral AI, которая превзошла Llama 2 13B по всем бенчмаркам, несмотря на меньший размер. Революционная модель, изменившая ландшафт открытых LLM.
- **Задачи**: Диалоги, следование инструкциям, генерация текста.
- **Особенности**: Sliding Window Attention (SWA), Grouped-Query Attention (GQA), контекст 8K токенов.
- **Значимость**: Доказала, что небольшие модели могут конкурировать с крупными при правильной архитектуре.

### Gemma 2 9B IT
- **ID**: `google/gemma-2-9b-it`
- **Дата выхода**: 27 июня 2024
- **Разработчик**: Google
- **Параметры**: 9 миллиардов
- **Описание**: Лёгкая открытая модель от Google, созданная на основе технологий Gemini. Превосходит Llama 3 8B в своём классе.
- **Задачи**: Генерация текста, ответы на вопросы, суммаризация, рассуждения.
- **Обучение**: 8 триллионов токенов, knowledge distillation от модели 27B.
- **Особенности**: Гибридное внимание (локальное + глобальное), Grouped Query Attention, оптимизация для edge-устройств.

### Llama 3.2 3B Instruct
- **ID**: `meta-llama/Llama-3.2-3B-Instruct`
- **Дата выхода**: 25 сентября 2024
- **Разработчик**: Meta
- **Параметры**: 3 миллиарда
- **Описание**: Компактная модель для работы на мобильных устройствах и edge. Контекст 128K токенов.
- **Задачи**: Суммаризация, следование инструкциям, перезапись текста, вызов функций (tool calling), локальная обработка на устройстве.
- **Обучение**: 9 триллионов токенов, knowledge distillation от Llama 3.1 8B и 70B.
- **Особенности**: Работает локально с сохранением приватности, 8 языков, превосходит Gemma 2 2.6B и Phi 3.5-mini.

### DeepSeek R1
- **ID**: `deepseek-ai/DeepSeek-R1`
- **Дата выхода**: 20 января 2025
- **Разработчик**: DeepSeek (Китай)
- **Параметры**: 671 миллиард (37B активных на запрос)
- **Описание**: Модель, ориентированная на рассуждения (reasoning). Конкурент OpenAI o1 с существенно меньшей стоимостью.
- **Задачи**: Логический вывод, математические задачи, сложные рассуждения, самопроверка, исправление ошибок.
- **Обучение**: SFT на chain-of-thought примерах + Reinforcement Learning.
- **Особенности**: Chain-of-thought reasoning, самоверификация, MIT лицензия.

### Qwen3 8B (САМАЯ НОВАЯ)
- **ID**: `Qwen/Qwen3-8B`
- **Дата выхода**: 29 апреля 2025
- **Разработчик**: Alibaba Cloud Qwen Team (Китай)
- **Параметры**: 8 миллиардов
- **Описание**: Новейшая модель семейства Qwen с поддержкой 119 языков и диалектов. Гибридный подход с двумя режимами работы.
- **Задачи**: Многоязычная генерация, кодинг, математика, рассуждения, диалоги.
- **Обучение**: До 36 триллионов токенов для крупных версий семейства.
- **Особенности**:
  - **Thinking Mode** - для сложных задач с цепочкой рассуждений
  - **Non-Thinking Mode** - быстрые ответы для простых вопросов
  - 119 языков, Apache 2.0 лицензия
- **Значимость**: Конкурирует с DeepSeek-R1, OpenAI o1, Grok-3 на задачах кодинга и математики.

---

## Хронология выхода моделей (в приложении)

| Модель | Дата выхода | Разработчик | API |
|--------|-------------|-------------|-----|
| GigaChat | Апрель 2023 | Sberbank | GigaChat |
| **Mistral 7B Instruct** | **Сентябрь 2023** | Mistral AI | Hugging Face |
| GigaChat Pro | 2024 | Sberbank | GigaChat |
| GigaChat Max | 2024 | Sberbank | GigaChat |
| Gemma 2 9B IT | Июнь 2024 | Google | Hugging Face |
| Llama 3.2 3B Instruct | Сентябрь 2024 | Meta | Hugging Face |
| DeepSeek R1 | Январь 2025 | DeepSeek | Hugging Face |
| **Qwen3 8B** | **Апрель 2025** | Alibaba | Hugging Face |

---

## Рекомендации по выбору

### Для русского языка
- **GigaChat Max** - лучший выбор для задач на русском языке, первое место в бенчмарке MERA

### Для рассуждений и математики
- **DeepSeek R1** - специализированная модель для логических задач, chain-of-thought reasoning
- **Qwen3 8B** - новейшая модель с Thinking Mode для сложных задач

### Для мобильных устройств и edge
- **Llama 3.2 3B Instruct** - компактная (3B параметров), работает локально с сохранением приватности
- **Mistral 7B Instruct** - эффективная 7B модель с низкими требованиями

### Для многоязычных задач
- **Qwen3 8B** - поддержка 119 языков и диалектов

### Универсальные задачи
- **Gemma 2 9B IT** - отличный баланс размера и качества, от Google
- **GigaChat Pro** - хороший баланс качества и скорости для русского языка

### Историческое сравнение
- **Mistral 7B** (2023) vs **Qwen3 8B** (2025) - демонстрирует прогресс открытых LLM за 1.5 года

---

## Источники

- [Sber GigaChat 2.0](https://aijourn.com/sber-presents-gigachat-2-0-the-strongest-neural-network-model-in-russian/)
- [GigaChat Wikipedia](https://en.wikipedia.org/wiki/GigaChat)
- [Mistral 7B Announcement](https://mistral.ai/news/announcing-mistral-7b/)
- [Mistral 7B on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- [Google Gemma 2](https://blog.google/technology/developers/google-gemma-2/)
- [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- [DeepSeek Wikipedia](https://en.wikipedia.org/wiki/DeepSeek)
- [DeepSeek R1 Overview](https://fireworks.ai/blog/deepseek-r1-deepdive)
- [Qwen3 Announcement](https://qwenlm.github.io/blog/qwen3/)
- [Qwen3 on Hugging Face](https://huggingface.co/Qwen/Qwen3-8B)
